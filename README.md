# Awesome Segment Anything
## News :boom:"Segment Anything" has been accepted by ICCV 2023:boom:

## Welcome to summit your project [[here](https://docs.google.com/spreadsheets/d/1AdOc_mZrkKP7XoKL9g7YO4EEpjlxdxEDK2yOdRZ_edg/edit?usp=sharing)]
## Survey
- **The First Comprehensive SAM Survey:** Chunhui Zhang, Li Liu, Yawen Cui, Guanjie Huang, Weilin Lin, Yiqian Yang, Yuehong Hu.<br />
  "A Comprehensive Survey on Segment Anything Model for Vision and Beyond." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.08196)] 
  [[homepage]( https://github.com/liliu-avril/Awesome-Segment-Anything)][[中文解读](https://mp.weixin.qq.com/s/uYpRzvRp22-40x8e0pLVIg)] 


## Papers
### :star2: Recommendations :star2:
- **SAM:** Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick.<br />
  "Segment Anything." **ICCV (2023)**.
  [[paper](https://arxiv.org/abs/2304.02643)] 
  [[homepage](https://segment-anything.com/)] 
  [[code](https://github.com/facebookresearch/segment-anything)]
  [[Zhihu](https://www.zhihu.com/question/593914819)]
  [[Reddit](https://www.reddit.com/r/singularity/comments/12cq56n/meta_ai_has_released_both_the_model_and_the/)]

- **SEEM:** Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, Yong Jae Lee.<br />
  "Segment Everything Everywhere All at Once." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2304.06718)] 
  [[code](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)]
  
- **SegGPT:** Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang.<br />
  "SegGPT: Segmenting Everything In Context." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2304.03284)] 
  [[code](https://github.com/baaivision/Painter)]

- **Grounding DINO:** Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang.<br />
  "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2303.05499)] 
  [[code]( https://github.com/IDEA-Research/GroundingDINO)]

- **OVSeg:** Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu.<br />
  "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP." CVPR (2023).
  [[paper](https://arxiv.org/abs/2210.04150)] 
[[homepage]( https://jeff-liangf.github.io/projects/ovseg/)] 
  [[code](https://github.com/facebookresearch/ov-seg)]

- **OneFormer:** Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.<br />
  "OneFormer: One Transformer to Rule Universal Image Segmentation." CVPR (2023).
  [[paper]( https://arxiv.org/abs/2211.06220)] 
[[homepage](https://praeclarumjj3.github.io/oneformer)] 
  [[code](https://github.com/SHI-Labs/OneFormer)]
  
- **ImageBind:** Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra.<br />
  "ImageBind: One Embedding Space To Bind Them All." CVPR (2023).
  [[paper](https://arxiv.org/abs/2305.05665)] 
  [[homepage](https://imagebind.metademolab.com/)] 
  [[code](https://github.com/facebookresearch/ImageBind)]

- **RAM:** Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, Yandong Guo, Lei Zhang.<br />
  "Recognize Anything: A Strong Image Tagging Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.03514)] 
  [[homepage](https://recognize-anything.github.io/)] 
  [[code](https://github.com/xinyu1205/Recognize_Anything-Tag2Text)]

- **PACGen:** Yuheng Li, Haotian Liu, Yangming Wen, Yong Jae Lee.<br />
  "Generate Anything Anywhere in Any Scene." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.17154)] 
 [[homepage](https://github.com/Yuheng-Li/PACGen)] 
  [[code](https://yuheng-li.github.io/PACGen/)]

- **OpenSeeD:** Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lei Zhang.<br />
  "A Simple Framework for Open-Vocabulary Segmentation and Detection." ICCV (2023).
  [[paper](https://arxiv.org/abs/2303.08131)] 
  [[code](https://github.com/IDEA-Research/OpenSeeD)]

- **ASM:** Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, Yu Qiao.<br />
  "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.01907)] 
  [[homepage](https://github.com/OpenGVLab/All-Seeing)] 
  [[demo](https://huggingface.co/spaces/OpenGVLab/all-seeing)]

- **Meta-Transformer:** Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue.<br />
  "Meta-Transformer: A Unified Framework for Multimodal Learning." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.10802)] 
  [[homepage](https://github.com/invictus717/MetaTransformer)]
  [[code](https://github.com/invictus717/MetaTransformer)]
  [[中文解读](https://mp.weixin.qq.com/s/r38bzqdJxDZUvtDI0c9CEw?poc_token=HJBW1GSjrlLI_fXbnZeCfAefBIyL3OT0__QH-hfc)]



### Follow-up Papers $\color{Magenta}(Last &ensp; updated &ensp; on: &ensp; 14 &ensp; August &ensp; 2023)$ 
:boom:**FoodSAM:** Xing Lan, Jiayi Lyu, Hanyu Jiang, Kun Dong, Zehai Niu, Yi Zhang, Jian Xue.<br />
  "FoodSAM: Any Food Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.05938)] 
  [[code](https://github.com/jamesjg/FoodSAM)]

:boom:**SAM-L:** Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo.<br />
  "Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.05785)] 

- **FAn :** Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus.<br />
  "Follow Anything: Open-set detection, tracking, and following in real-time." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.05737)] 
  [[code](https://github.com/alaamaalouf/FollowAnything)]
  [[demo](https://www.youtube.com/watch?v=6Mgt3EPytrw)]

- **SSOM:** Ruikai Cui, Siyuan He, Shi Qiu.<br />
  "Adaptive Low Rank Adaptation of Segment Anything to Salient Object Detection." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.05426)] 

- **AquaSAM:** Muduo Xu, Jianhao Su, Yutao Liu.<br />
  "AquaSAM: Underwater Image Foreground Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.04218)] 

- **AdaptiveSAM:** Jay N. Paranjape, Nithin Gopalakrishnan Nair, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel.<br />
  "AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.03726)] 
  [[code](https://github.com/JayParanjape/biastuning)]

- Ziyi Huang, Hongshan Liu, Haofeng Zhang, Fuyong Xing, Andrew Laine, Elsa Angelini, Christine Hendon, Yu Gan.<br />
  "Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.00883)] 

- Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister.<br />
  "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2308.00675)] 

- **SAMFlow:** Shili Zhou, Ruian He, Weimin Tan, Bo Yan.<br />
  "SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.16586)] 

- **HQTrack:** Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, Hanyuan Chen, Chenyang Li.<br />
  "Tracking Anything in High Quality." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.13974)] 
  [[code](https://github.com/jiawen-zhu/HQTrack)]

- **Fashion Matrix:** Zheng Chong, Xujie Zhang, Fuwei Zhao, Zhenyu Xie, Xiaodan Liang.<br />
  "Fashion Matrix: Editing Photos by Just Talking." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.13240)] 
  [[homepage](https://zheng-chong.github.io/FashionMatrix/)]
  [[code](https://github.com/Zheng-Chong/FashionMatric)]

- **RoboChop:** Atharva Dikshit, Alison Bartsch, Abraham George, Amir Barati Farimani.<br />
  "RoboChop: Autonomous Framework for Fruit and Vegetable Chopping Leveraging Foundational Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.13159)] 

- **Industrial-SA:** Keno Moenck, Arne Wendt, Philipp Prünte, Julian Koch, Arne Sahrhage, Johann Gierecker, Ole Schmedemann, Falko Kähler, Dirk Holst, Martin Gomse, Thorsten Schüppstuhl, Daniel Schoepflin.<br />
  "Industrial Segment Anything -- a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.12674)] 


- **CNOS:** Van Nguyen Nguyen, Tomas Hodan, Georgy Ponimatkin, Thibault Groueix, Vincent Lepetit.<br />
  "CNOS: A Strong Baseline for CAD-based Novel Object Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.11067)] 
  [[code](https://github.com/nv-nguyen/cnos)]

- **SAM-Path:** Jingwei Zhang, Ke Ma, Saarthak Kapse, Joel Saltz, Maria Vakalopoulou, Prateek Prasanna, Dimitris Samaras.<br />
  "SAM-Path: A Segment Anything Model for Semantic Segmentation in Digital Pathology." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.09570)] 

- **BuboGPT:** Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, Bingyi Kang.<br />
  "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.08581)] 
  [[code](https://bubo-gpt.github.io)]

- **OpenSU:** Ruiping Liu, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ke Cao, Yufan Chen, Kailun Yang, Rainer Stiefelhagen.<br />
  "Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.07757)] 
  [[code](https://github.com/RuipingL/OpenSU)]

- **OG:** Zichao Dong, Hang Ji, Weikun Zhang, Xufeng Huang, Junbo Chen.<br />
  "OG: Equip vision occupancy with instance segmentation and visual grounding." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.05873)] 

- **$SAM^{Med}$:** Chenglong Wang, Dexuan Li, Sucheng Wang, Chengxiu Zhang, Yida Wang, Yun Liu, Guang Yang.<br />
  $SAM^{Med}$: A medical image annotation framework based on large vision model. ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.05617)] 

- **SAM-U:** Guoyao Deng, Ke Zou, Kai Ren, Meng Wang, Xuedong Yuan, Sancong Ying, Huazhu Fu.<br />
  "SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image." ArXiv (2023).
  [[paper]()] 

- **Semantic-SAM:** Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, Jianfeng Gao.<br />
  "Semantic-SAM: Segment and Recognize Anything at Any Granularity." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.04767)] 
  [[code](https://github.com/UX-Decoder/Semantic-SAM)]

- **SAM-IQA:** Xinpeng Li, Ting Jiang, Haoqiang Fan, Shuaicheng Liu.<br />
  "SAM-IQA: Can Segment Anything Boost Image Quality Assessment?." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.04455)] 
  [[code](https://github.com/Hedlen/SAM-IQA)]

- **Cross-SAM:** Xiaoyu Bai, Fan Bai, Xiaofei Huo, Jia Ge, Tony C. W. Mok, Zi Li, Minfeng Xu, Jingren Zhou, Le Lu, Dakai Jin, Xianghua Ye, Jingjing Lu, Ke Yan.<br />
  "Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images." ArXiv (2023).
  [[paper]()] 

- **LAM-SC:** Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You.<br />
  "Large AI Model-Based Semantic Communications." ArXiv (2023).
  [[paper]()] 

- **MSDeAOT:** Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang.<br />
  "ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.02508)] 

- **EM-SAM:** Ao Cheng, Guoqiang Zhao, Lirong Wang, Ruobing Zhang.<br />
  "AxonCallosumEM Dataset: Axon Semantic Segmentation of Whole Corpus Callosum cross section from EM Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.02464)] 

- **SAM-PT:** Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, Fisher Yu.<br />
  "Segment Anything Meets Point Tracking." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.01197)] 
  [[code](https://github.com/SysCV/sam-pt)]

- **SAMAug:** Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, Quanzheng Li, Tianming Liu, Xiang Li.<br />
  "SAMAug: Point Prompt Augmentation for Segment Anything Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.01187)] 
  [[code](XXXXXXXXXXXXXXXXXXXXXX)]

- **SAM-DA:** Liangliang Yao, Haobo Zuo, Guangze Zheng, Changhong Fu, Jia Pan.<br />
  "SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.01024)] 
  [[code](https://github.com/vision4robotics/SAM-DA)]

- **RefSAM:** Yonglin Li, Jing Zhang, Xiao Teng, Long Lan.<br />
  "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.00997)] 
  [[code](https://github.com/LancasterLi/RefSAM)]

- **All-in-SAM:** Can Cui, Ruining Deng, Quan Liu, Tianyuan Yao, Shunxing Bao, Lucas W. Remedios, Yucheng Tang, Yuankai Huo.<br />
  "All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.00290)] 
  

- Zenglin Shi, Ying Sun, Mengmi Zhang.<br />
  "Training-free Object Counting with Prompts." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.00038)] 
  [[code](https://github.com/shizenglin/training-free-object-counter)]

- Xiaoyu Shi, Shurong Chai, Yinhao Li, Jingliang Cheng, Jie Bai, Guohua Zhao, Yen-Wei Chen.<br />
  "Cross-modality Attention Adapter: A Glioma Segmentation Fine-tuning Method for SAM Using Multimodal Brain MR Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2307.01124)] 


- **TDA:** Ruben Glatt, Shusen Liu.<br />
  "Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.17400)] 

- **DADF:** Yingxin Lai, Zhiming Luo, Zitong Yu.<br />
  "Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.17075)] 
  [[code](https://github.com/laiyingxin2/DADF)]

- Lucas Prado Osco, Qiusheng Wu, Eduardo Lopes de Lemos, Wesley Nunes Gonçalves, Ana Paula Marques Ramos, Jonathan Li, José Marcato Junior.<br />
  "The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.16623)] 

- **RSPrompter:** Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, Zhenwei Shi.<br />
  "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.16269)] 
  [[code](https://kyanchen.github.io/RSPrompter)]

- Zhewei Chen, Wai Keung Wong, Zuofeng Zhong, Jinpiao Liao, Ying Qu.<br />
  "Effective Transfer of Pretrained Large Visual Model for Fabric Defect Segmentation via Specifc Knowledge Injection." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.16186)] 

- Zheyan Jin, Shiqi Chen, Yueting Chen, Zhihai Xu, Huajun Feng.<br />
  "Let Segment Anything Help Image Dehaze." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.15870)] 

- **CLIP-SAM:** Evan Kellener, Ihina Nath, An Ngo, Thomas Nguyen, Joshua Schuman, Coen Adler, Arnav Kartikeya.<br />
  "Utilizing Segment Anything Model For Assessing Localization of GRAD-CAM in Medical Imaging." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.15692)] 
 
- **MESS:** Benedikt Blumenstiel, Johannes Jakubik, Hilde Kühne, Michael Vössing.<br />
  "What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.15521)] 
  [[code](https://github.com/blumenstiel/MESS)]

- **MMPM:** Jiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, Limin Wang.<br />
  "Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.05716)] 
  [[YouTube](https://www.youtube.com/watch?v=1m9wNzfp_4E)]
  [[Bilibili](https://www.bilibili.com/video/BV178411Z7H2/)]

- **CellViT:** Fabian Hörst, Moritz Rempe, Lukas Heine, Constantin Seibold, Julius Keyl, Giulia Baldini, Selma Ugurel, Jens Siveke, Barbara Grünwald, Jan Egger, Jens Kleesiek.<br />
  "CellViT: Vision Transformers for Precise Cell Segmentation and Classification." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.15350)] 
  [[code](https://github.com/TIO-IKIM/CellViT)]

- **MedLSAM:** Wenhui Lei, Xu Wei, Xiaofan Zhang, Kang Li, Shaoting Zhang.<br />
  "MedLSAM: Localize and Segment Anything Model for 3D Medical Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.14752)] 
  [[code](https://github.com/openmedlab/MedLSAM)]

- **MobileSAM:** Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, Choong Seon Hong.<br />
  "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.14289)] 
  [[code](https://github.com/ChaoningZhang/MobileSAM)]

- **SonarSAM:** Lin Wang, Xiufen Ye, Liqiang Zhu, Weijie Wu, Jianguo Zhang, Huiming Xing, Chao Hu.<br />
  "When SAM Meets Sonar Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.14109)] 
  [[code](https://github.com/wangsssky/SonarSAM)]

- **AutoSAM:** Xinrong Hu, Xiaowei Xu, Yiyu Shi.<br />
  "How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.13731)] 
  [[code](https://github.com/xhu248/AutoSAM)]

- **3DSAM-adapter:** Shizhan Gong, Yuan Zhong, Wenao Ma, Jinpeng Li, Zhao Wang, Jingyang Zhang, Pheng-Ann Heng, Qi Dou.<br />
  "3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.13465)] 
  [[code](https://github.com/med-air/3DSAM-adapter)]

- Xinru Shan, Chaoning Zhang.<br />
  "Robustness of Segment Anything Model (SAM) for Autonomous Driving in Adverse Weather Conditions." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.13290)] 

- **SAM-LST:** Shurong Chai, Rahul Kumar Jain, Shiyu Teng, Jiaqing Liu, Yinhao Li, Tomoko Tateyama, Yen-wei Chen.<br />
  "Ladder Fine-tuning approach for SAM integrating complementary network." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.12737)] 
  [[code](https://github.com/11yxk/SAM-LST)]

- Mohsen Ahmadi, Masoumeh Farhadi Nia, Sara Asgarian, Kasra Danesh, Elyas Irankhah, Ahmad Gholizadeh Lonbar, Abbas Sharifi.<br />
  "Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.12510)] 

- **FastSAM:** Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, Jinqiao Wang.<br />
  "Fast Segment Anything." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.12156)] 
  [[code](https://github.com/CASIA-IVA-Lab/FastSAM)]

- **Seal:** Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu.<br />
  "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.09347)] 
  [[code](https://github.com/youquanl/Segment-Any-Point-Cloud)]
  [[homepage](https://ldkong.com/Seal)]

- Lian Zhang, Zhengliang Liu, Lu Zhang, Zihao Wu, Xiaowei Yu, Jason Holmes, Hongying Feng, Haixing Dai, Xiang Li, Quanzheng Li, Dajiang Zhu, Tianming Liu, Wei Liu.<br />
  "Segment Anything Model (SAM) for Radiation Oncology." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.11730)] 

- **Enlighten-Anything:** Qihan Zhao, Xiaofeng Zhang, Hao Tang, Chaochen Gu, Shanying Zhu.<br />
  "Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.10286)] 
  [[code](https://github.com/zhangbaijin/enlighten-anything)]

- **SAA+:** Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Liang Gao, Weiming Shen.<br />
  "Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection." CVPR2023 Workshop.
  [[paper](https://arxiv.org/abs/2306.09067)] 
  [[code](https://github.com/caoyunkang/Segment-Any-Anomaly)]

- **TEPO:** Chuyun Shen, Wenhao Li, Ya Zhang, Xiangfeng Wang.<br />
  "Temporally-Extended Prompts Optimization for SAM in Interactive Medical Image Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.08958)] 

- **TomoSAM:** Federico Semeraro, Alexandre Quintart, Sergio Fraile Izquierdo, Joseph C. Ferguson.<br />
  "TomoSAM: a 3D Slicer extension using SAM for tomography segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.08609)] 
  [[code](https://github.com/fsemerar/SlicerTomoSAM)]
  
- Madeline Chantry Schiappa, Sachidanand VS, Yunhao Ge, Ondrej Miksik, Yogesh S. Rawat, Vibhav Vineet.<br />
  "Robustness Analysis on Foundational Segmentation Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.09278)] 
  [[code](https://tinyurl.com/fm-robust)]

  
- Yu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, Choong Seon Hong.<br />
  "Robustness of SAM: Segment Anything Under Corruptions and Beyond." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.07713)] 

- **AutoSAM:** Tal Shaharabany, Aviad Dahan, Raja Giryes, Lior Wolf.<br />
  "AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.06370)] 
 
- **SAM-shadow:** Xiaofeng Zhang, Chaochen Gu, Shanying Zhu.<br />
  "SAM-helps-Shadow:When Segment Anything Model meet shadow removal." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.06113)] 
  [[code]( https://github.com/zhangbaijin/SAM-helps-Shadow)]

- Chaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois Rameau, Sung-Ho Bae, Choong Seon Hong.<br />
  "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.06211)]

- **MAM:** Jiachen Li, Jitesh Jain, Humphrey Shi.<br />
  "Matting Anything." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.05399)] 
  [[code](https://chrisjuniorli.github.io/project/Matting-Anything/)]

-  Haochen Xue, Mingyu Jin, Chong Zhang, Yuxuan Huang, Qian Weng, Xiaobo Jin.<br />
  "Automatic Image Blending Algorithm Based on SAM and DINO." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.05382)] 


- **MatAny:** Jingfeng Yao, Xinggang Wang, Lang Ye, Wenyu Liu.<br />
  "Matte Anything: Interactive Natural Image Matting with Segment Anything Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.04121)] 
  [[code](https://github.com/hustvl/Matte-Anything)]

- **CNS:** Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, Wenping Wang.<br />
  "Towards Label-free Scene Understanding by Vision Foundation Models." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.03899)] 
  [[code](https://github.com/runnanchen/Label-Free-Scene-Understanding)]

- **SAM3D:** Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, Xihui Liu.<br />
  "SAM3D: Segment Anything in 3D Scenes." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.03908)] 
  [[code](https://github.com/Pointcept/SegmentAnything3D)]

- **Calib-Anything:** Zhaotong Luo, Guohang Yan, Yikang Li.<br />
  " Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2306.02656)] 
  [[code](https://github.com/OpenCalib/CalibAnything)]

-  Shijie Chang, Zeqi Hao, Ben Kang, Xiaoqi Zhao, Jiawen Zhu, Zhenyu Chen, Lihe Zhang, Lu Zhang, Huchuan Lu.<br />
  " 3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2306.02291)] 

- **USD:** Yulin He, Wei Chen, Yusong Tan, Siqi Wang.<br />
  " USD: Unknown Sensitive Detector Empowered by Decoupled Objectness and Segment Anything Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2306.02275)] 

- **SAM3D:** Dingyuan Zhang, Dingkang Liang, Hongcheng Yang, Zhikang Zou, Xiaoqing Ye, Zhe Liu, Xiang Bai.<br />
  "SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2306.02245)] 
  [[code]( https://github.com/DYZhang09/SAM3D)]

-  Shehbaz Tariq, Brian Estadimas Arfeto, Chaoning Zhang, Hyundong Shin.<br />
  "Segment Anything Meets Semantic Communication." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.02094)] 

- **HQ-SAM:** Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu.<br />
  "Segment Anything in High Quality." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2306.01567)] 
  [[code](https://github.com/SysCV/SAM-HQ)]


- **DeSAM:** Yifan Gao, Wei Xia, Dingdu Hu, Xin Gao.<br />
  "DeSAM: Decoupling Segment Anything Model for Generalizable Medical Image Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2306.00499)] 
  [[code]( https://github.com/yifangao112/DeSAM)]
 
 
- **FineRewards:** Guian Fang, Zutao Jiang, Jianhua Han, Guangsong Lu, Hang Xu, Xiaodan Liang.<br />
  "Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.19599)] 
  [[code](https://github.com/Enderfga/FineRewards)]


- **InstructEdit:** Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka.<br />
  "InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.18047)] 
  [[code]( https://github.com/QianWangX/InstructEdit)]

- **AIMS:** Lu Qi, Jason Kuen, Weidong Guo, Jiuxiang Gu, Zhe Lin, Bo Du, Yu Xu, Ming-Hsuan Yang.<br />
  "AIMS: All-Inclusive Multi-Level Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.17768)] 
  [[code](https://github.com/dvlab-research/Entity)]

- **ShadowSAM:** Yonghui Wang, Wengang Zhou, Yunyao Mao, Houqiang Li.<br />
  "Detect Any Shadow: Segment Anything for Video Shadow Detection." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.16698)] 
[[code](https://github.com/harrytea/Detect-AnyShadow)]


- **ISA-NeRF:** Xiaokang Chen, Jiaxiang Tang, Diwen Wan, Jingbo Wang, Gang Zeng.<br />
  "Interactive Segment Anything NeRF with Feature Imitation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.16233)] 
  [[homepage]( https://me.kiui.moe/san/)]

- Yihao Huang, Yue Cao, Tianlin Li, Felix Juefei-Xu, Di Lin, Ivor W. Tsang, Yang Liu, Qing Guo.<br />
  "On the Robustness of Segment Anything." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.16220)] 

- **SAMScore:** Yunxiang Li, Meixu Chen, Wenxuan Yang, Kai Wang, Jun Ma, Alan C. Bovik, You Zhang.<br />
  "SAMScore: A Semantic Structural Similarity Metric for Image Translation Evaluation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.15367)] 
  [[code]( https://github.com/Kent0n-Li/SAMScore)]

- **SAD:** Jun Cen, Yizheng Wu, Kewei Wang, Xingyi Li, Jingkang Yang, Yixuan Pei, Lingdong Kong, Ziwei Liu, Qifeng Chen.<br />
  "SAD: Segment Any RGBD." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.14207)] 
  [[code](https://github.com/Jun-CEN/SegmentAnyRGBD)]

- **SPT:** Zeyu Xiao, Jiawang Bai, Zhihe Lu, Zhiwei Xiong.<br />
  "A Dive into SAM Prior in Image Restoration." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.13620)] 


- **Matcher:** Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen.<br />
  "Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.13310)] 
  [[code](https://github.com/aim-uofa/Matcher)]


- **RAP:** Jiaxi Jiang, Christian Holz.<br />
  "Restore Anything Pipeline: Segment Anything Meets Image Restoration." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.13093)] 
  [[code](https://github.com/eth-siplab/RAP)]


- **UVOSAM:** Zhenghao Zhang, Zhichao Wei, Shengfan Zhang, Zuozhuo Dai, Siyu Zhu.<br />
  "UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.12659)] 


- **BreastSAM:** Mingzhe Hu, Yuheng Li, Xiaofeng Yang.<br />
  "BreastSAM: A Study of Segment Anything Model for Breast Tumor Detection in Ultrasound Images." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.12447)] 


- **SAMSh:** Leiping Jie, Hui Zhang.<br />
  "When SAM Meets Shadow Detection." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.11513)] 
  [[code](https://github.com/LeipingJie/SAMSh)]

- **Instruct2Act:** Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li.<br />
  "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.11176)] 
  [[code](https://github.com/OpenGVLab/Instruct2Act)]


- **WS-SAM:** Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li.<br />
  "Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.11003)] 


- **SAA+:** Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, Weiming Shen.<br />
  "Segment Any Anomaly without Training via Hybrid Prompt Regularization." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.10724)] 
  [[code](https://github.com/caoyunkang/Segment-Any-Anomaly)]


- **OR-NeRF:** Youtan Yin, Zhoujie Fu, Fan Yang, Guosheng Lin.<br />
  "OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.10503)] 


- **PromptUNet:** Junde Wu.<br />
  "PromptUNet: Toward Interactive Medical Image Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.10300)] 
  [[code](https://github.com/WuJunde/PromptUNet)]


- **EAC:** Ao Sun, Pingchuan Ma, Yuanyuan Yuan, Shuai Wang.<br />
  "Explain Any Concept: Segment Anything Meets Concept-Based Explanation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.10289)] 

- Xiao Yang, Haixing Dai, Zihao Wu, Ramesh Bist, Sachin Subedi, Jin Sun, Guoyu Lu, Changying Li, Tianming Liu, Lilong Chai.<br />
  "SAM for Poultry Science." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.10254)] 


- **Leaf Only SAM:** Dominic Williams, Fraser MacFarlane, Avril Britten.<br />
  "Leaf Only SAM: A Segment Anything Pipeline for Zero-Shot Automated Leaf Segmentation." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.09418)] 

- **KD-SAM:** Sahib Julka, Michael Granitzer.<br />
  "Knowledge distillation with Segment Anything (SAM) model for Planetary Geological Mapping." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.07586)] 

- **SAM-Track:** Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, Yi Yang.<br />
  "Segment-and-Track Anything." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2305.06558)] 
  [[code](https://github.com/z-x-yang/Segment-and-Track-Anything)]

- **SEEM:** Zhihe Lu, Zeyu Xiao, Jiawang Bai, Zhiwei Xiong, Xinchao Wang.<br />
  "Can SAM Boost Video Super-Resolution?" ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.06524)] 
  
-  Yuqing Wang, Yun Zhao, Linda Petzold.<br />
  "An Empirical Study on the Robustness of the Segment Anything Model (SAM)." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.06422)] 

- **SAM-WSSS:** Tianle Chen, Zheda Mai, Ruiwen Li, Wei-lun Chao.<br />
  "Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.05803)] 
  [[code]( https://github.com/cskyl/SAM_WSSS)]

- **SAM4MIS:** Yichi Zhang, Rushi Jiao.<br />
  "How Segment Anything Model (SAM) Boost Medical Image Segmentation?" ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.03678)] 
  [[code]( https://github.com/YichiZhang98/SAM4MIS)]

- **BadSAM:** Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu.<br />
  "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.03289)] 

- **PerSAM:** Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li.<br />
  "Personalize Segment Anything Model with One Shot." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.03048)] 
  [[code]( https://github.com/ZrrSkywalker/Personalize-SAM)]

- **CAT:** Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, Shanshan Zhao.<br />
  "Caption Anything: Interactive Image Description with Diverse Multimodal Controls." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.02677)] 
  [[code]( https://github.com/ttengwang/Caption-Anything)]

- **SAMRS:** Di Wang, Jing Zhang, Bo Du, Dacheng Tao, Liangpei Zhang.<br />
  "Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.02034)] 
  [[code]( https://github.com/ViTAE-Transformer/SAMRS)]

- **AV-SAM:** Shentong Mo, Yapeng Tian.<br />
  "AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.01836)] 

- **WSSS:** Weixuan Sun, Zheyuan Liu, Yanhao Zhang, Yiran Zhong, Nick Barnes.<br />
  "An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.01586)] 

- **PLG-SAM:** Peng-Tao Jiang, Yuqi Yang.<br />
  "Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.01275)] 

- **Attack-SAM:** Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, In So Kweon.<br />
  "Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00866)] 

- **Polyp-SAM:** Yuheng Li, Mingzhe Hu, Xiaofeng Yang.<br />
  "Polyp-SAM: Transfer SAM for Polyp Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00293)] 
  [[code]( https://github.com/ricklisz/Polyp-SAM)]

- Dongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, Choong Seon Hong.<br />
  "Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00278)] 

- **DSEC-MOS:** Zhuyun Zhou, Zongwei Wu, Rémi Boutteau, Fan Yang, Dominique Ginhac.<br />
  "DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00126)] 
  [[code]( https://github.com/ZZY-Zhou/DSEC-MOS)]

- Christian Mattjie, Luis Vinicius de Moura, Rafaela Cappelari Ravazio, Lucas Silveira Kupssinskü, Otávio Parraga, Marcelo Mussi Delucis, Rodrigo Coelho Barros.<br />
  "Zero-shot performance of the Segment Anything Model (SAM) in 2D medical imaging: A comprehensive evaluation and practical guidelines." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00109)] 
  [[code]( https://github.com/Malta-Lab/SAM-zero-shot-in-Medical-Imaging)]

- Dongjie Cheng, Ziyuan Qin, Zekun Jiang, Shaoting Zhang, Qicheng Lao, Kang Li.<br />
  "SAM on Medical Images: A Comprehensive Study on Three Prompt Modes." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2305.00035)] 

- An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren.<br />
  "SAM Meets Robotic Surgery: An Empirical Study in Robustness Perspective." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.14674)] 

- Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, Haozhe Chi, Xindi Hu, Deng-Ping Fan, Fajin Dong, Dong Ni.<br />
  "Segment Anything Model for Medical Images?" ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.14660)] 

- **Edit Everything:** Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, Dong Yang, Fobo Shi, Xiaodong Lin.<br />
  "Edit Everything: A Text-Guided Generative System for Images Editing." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.14006)] 
  [[code]( https://github.com/DefengXie/Edit_Everything)]

- **SkinSAM:** Mingzhe Hu, Yuheng Li, Xiaofeng Yang.<br />
  "SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.13973)] 

- **GazeSAM:** Bin Wang, Armstrong Aboah, Zheyuan Zhang, Ulas Bagci.<br />
  " GazeSAM: What You See is What You Segment." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.13844)] 
  [[code]( https://github.com/ukaukaaaa/GazeSAM)]

- **SAMed:** Kaidong Zhang, Dong Liu.<br />
  " Customized Segment Anything Model for Medical Image Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.13785)] 
  [[code]( https://github.com/hitachinsk/SAMed)]

- **LearnablePromptSAM:** Zhongxi Qiu, Yan Hu, Heng Li, Jiang Liu.<br />
  " Learnable Ophthalmology SAM." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.13425)] 
  [[code]( https://github.com/Qsingle/LearnablePromptSAM)]

- Simiao Ren, Francesco Luzi, Saad Lahrichi, Kaleb Kassaw, Leslie M. Collins, Kyle Bradbury, Jordan M. Malof.<br />
  " Segment anything, from space?." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.13000)] 

-  Peilun Shi, Jianing Qiu, Sai Mu Dalike Abaxi, Hao Wei, Frank P. -W. Lo, Wu Yuan.<br />
  "Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.12637)] 

- **MSA:** Junde Wu, Yu Zhang, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin.<br />
  " Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.12620)] 
  [[code]( https://github.com/WuJunde/Medical-SAM-Adapter)]

- Mohsen Ahmadi, Ahmad Gholizadeh Lonbar, Abbas Sharifi, Ali Tarlani Beris, Mohammadsadegh Nouri, Amir Sharifzadeh Javidi.<br />
  "Application of Segment Anything Model for Civil Infrastructure Defect Assessment." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.12600)] 

- **SA3D:** Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian.<br />
  " Segment Anything in 3D with NeRFs." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.12308)] 
  [[code]( https://jumpat.github.io/SA3D/)]

- **MedSAM:** Jun Ma, Bo Wang.<br />
  " Segment Anything in Medical Images." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.12306)] 
  [[code]( https://github.com/bowang-lab/MedSAM)]

- **TAM:** Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng.<br />
  "Track Anything: Segment Anything Meets Videos." ArXiv (2023).
  [[paper](https://arxiv.org/abs/2304.11968)] 
  [[code](https://github.com/gaomingqi/Track-Anything)]

- **SNA:** Yongcheng Jing, Xinchao Wang, Dacheng Tao.<br />
  "Segment Anything in Non-Euclidean Domains: Challenges and Opportunities." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.11595)] 

- **SAMAug:** Yizhe Zhang, Tao Zhou, Peixian Liang, Danny Z. Chen.<br />
  "Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.11332)] 

- **Count-Anything:** Zhiheng Ma, Xiaopeng Hong, Qinnan Shangguan.<br />
  "Can SAM Count Anything? An Empirical Study on SAM Counting." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.10817)] 
  [[code]( https://github.com/Vision-Intelligence-and-Robots-Group/count-anything)]

- **Text2Seg:** Jielu Zhang, Zhongliang Zhou, Gengchen Mai, Lan Mu, Mengxuan Hu, Sheng Li.<br />
  " Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.10597)] 
  [[code]( https://github.com/Douglas2Code/Text2Seg)]

- Maciej A. Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, Yixin Zhang.<br />
  "Segment Anything Model for Medical Image Analysis: an Experimental Study." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.10517)] 

- **Anything-3D:** Qiuhong Shen, Xingyi Yang, Xinchao Wang.<br />
  "Anything-3D: Towards Single-view Anything Reconstruction in the Wild." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.10261)] 
  [[code]( https://github.com/Anything-of-anything/Anything-3D)]

- **Any-to-Any Transfer:** Songhua Liu, Jingwen Ye, Xinchao Wang.<br />
  "Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.09728)] 
  [[code]( https://github.com/Huage001/Transfer-Any-Style)]

- Sheng He, Rina Bao, Jingpeng Li, Jeffrey Stout, Atle Bjornerud, P. Ellen Grant, Yangming Ou.<br />
  "Computer-Vision Benchmark Segment-Anything Model (SAM) in Medical Images: Accuracy in 12 Datasets." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.09324)] 

- **SAM-Adapter:** Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, Yan Wang, Zejian Li, Lingyun Sun, Papa Mao, Ying Zang.<br />
  "SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.09148)] 

- Chuanfei Hu, Tianyi Xia, Shenghong Ju, Xinde Li.<br />
  " When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.08506)] 

- **SATIR:** Junzhang Chen, Xiangzhi Bai.<br />
  " Learning to "Segment Anything" in Thermal Infrared Images through Knowledge Distillation with a Large Scale Dataset SATIR." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.07969)] 
  [[code]( https://github.com/chenjzBUAA/SATIR)]

- Florian Putz, Johanna Grigo, Thomas Weissmann, Philipp Schubert, Daniel Hoefler, Ahmed Gomaa, Hassen Ben Tkhayat, Amr Hagag, Sebastian Lettmaier, Benjamin Frey, Udo S. Gaipl, Luitpold V. Distel, Sabine Semrau, Christoph Bert, Rainer Fietkau, Yixing Huang.<br />
  "The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.07875)] 

-  Iraklis Giannakis, Anshuman Bhardwaj, Lydia Sam, Georgios Leontidis.<br />
  " Deep learning universal crater detection using Segment Anything Model (SAM)." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.07764)] 

- **SAMPolyp:** Tao Zhou, Yizhe Zhang, Yi Zhou, Ye Wu, Chen Gong.<br />
  " Can SAM Segment Polyps?" ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.07583)] 
  [[code]( https://github.com/taozh2017/SAMPolyp)]

- **Inpaint-Anything:** Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, Zhibo Chen.<br />
  "Inpaint Anything: Segment Anything Meets Image Inpainting." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.06790)] 
  [[code]( https://github.com/geekyutao/Inpaint-Anything)]

- Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, Luc Van Gool.<br />
  " SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.06790)] 

- Wei Ji, Jingjing Li, Qi Bi, Wenbo Li, Li Cheng.<br />
  "Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.05750)] 

- **CLIP Surgery:** Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li.<br />
  "CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.05653)] 
  [[code]( https://github.com/xmed-lab/CLIP_Surgery)]

- **SAMM:** Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand, Mehran Armand.<br />
  "SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.05622)] 
  [[code]( https://github.com/bingogome/samm)]

- **SAM.MD:** Saikat Roy, Tassilo Wald, Gregor Koehler, Maximilian R. Rokuss, Nico Disch, Julius Holzschuh, David Zimmerer, Klaus H. Maier-Hein.<br />
  "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.05396)] 


- **SAM vs BET:** Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug.<br />
  "SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.04738)] 
  [[code](XXXXXXXXXXXXXXXXXXXXXX)]

- **SAMCOD:** Lv Tang, Haoke Xiao, Bo Li.<br />
  "Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection." ArXiv (2023).
  [[paper]( https://arxiv.org/abs/2304.04709)] 
  [[code]( https://github.com/luckybird1994/SAMCOD)]



## Open Source Projects
| No. | Project | Title | Project page | Code base | Affiliation | Description |
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| 001 | SAM | Segment Anything | [Project page](https://segment-anything.com/)  | [Code](https://github.com/facebookresearch/segment-anything) | Meta | A foundation model for general segmentation. |
| 002 | SAM-Track | Segment and Track Anything | [Colab](https://colab.research.google.com/drive/1R10N70AJaslzADFqb-a5OihYkllWEVxB?usp=sharing)  |  [Code](https://github.com/z-x-yang/Segment-and-Track-Anything)   |   Zhejiang University    |    A  project dedicated to tracking and segmenting any objects in videos, either automatically or interactively. |
| 003  |  Grounded-SAM  |  Grounded-Segment-Anything  |  [Colab](https://github.com/camenduru/grounded-segment-anything-colab)   |  [Code](https://github.com/IDEA-Research/Grounded-Segment-Anything)    |  IDEA-Research   |    A project by combining Grounding DINO and SAM which aims to detect and segment Anything with text inputs. |
| 004  |  MMDet-SAM  |  -  |  -   |  [Code](https://github.com/open-mmlab/playground/tree/main/mmdet_sam)    |   OpenMMLab    |    A new way of instance segmentation by combining SAM with Closed-Set Object Detection, Open-Vocabulary Object Detection, Grounding Object Detection. |
| 005  |  MMRotate-SAM  |  Zero-shot Oriented Object Detection with SAM  |  -   |  [Code](https://github.com/open-mmlab/playground/tree/main/mmrotate_sam)    |   OpenMMLab    |    A project join SAM and weakly supervised horizontal box detection to achieve rotated box detection. |
| 006  |  MMOCR-SAM  |  -  |  -   |  [Code](https://github.com/open-mmlab/playground/tree/main/mmocr_sam)    |   OpenMMLab    |    A solution of Text Detection/Recognition and SAM that segments every text character, with striking text removal and text inpainting demos driven by diffusion models and Gradio. |
| 007  |  MMEditing-SAM  |  -  |  -   |  [Code](https://github.com/open-mmlab/playground/tree/main/mmagic_sam)    |   OpenMMLab    |    A project join SAM and image generation to create awesome images and edit any part of them. |
|  008  |  Label-Studio-SAM  |  OpenMMLab PlayGround: Semi-Automated Annotation with Label-Studio and SAM  |  -   |  [Code](https://github.com/open-mmlab/playground/tree/main/label_anything)    |   OpenMMLab     |    A project combining Label-Studio and SAM to achieve semi-automated annotation. |
|  009  |  PaddleSeg  |  Segment Anything with PaddleSeg  |  -   |  [Code](https://github.com/PaddlePaddle/PaddleSeg/tree/release/2.8/contrib/SegmentAnything)   |   PaddlePaddle   |    A pretrained model parameters of PaddlePaddle format.|
| 010  |  SegGPT  |  Segmenting Everything In Context  |  [Hugging Face](https://huggingface.co/spaces/BAAI/SegGPT)   |  [Code](https://github.com/baaivision/Painter)    |   BAAI-Vision    |    SAM In Context based on Painter. |
| 011  |  SEEM  |  Segment Everything Everywhere All at Once  |  [Hugging Face](https://huggingface.co/spaces/xdecoder/SEEM)   |  [Code](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)    |   Microsoft    |    A project can Segment Everything Everywhere with Multi-modal prompts all at once. |
| 012  |  CLIP Surgery  |  CLIP Surgery for Better Explainability with Enhancement in Open Vocabulary Tasks   |  [Project page](https://github.com/xmed-lab/CLIP_Surgery/blob/master/demo.ipynb)   |  [Code](https://github.com/xmed-lab/CLIP_Surgery)    |   HKUST    |    A work about SAM based on CLIP's explainability to achieve text to mask without manual points. |
| 013  |  SAMCOD  | Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection |-|[Code](https://github.com/luckybird1994/SAMCOD)|-|SAM +Camouflaged object detection (COD) task.|
| 014  |  Inpaint Anything  |  Segment Anything Meets Image Inpainting  |  [Hugging Face](https://huggingface.co/spaces/InpaintAI/Inpaint-Anything)  |  [Code](https://github.com/geekyutao/Inpaint-Anything)    |   USTC and EIT    |    SAM combines Inpainting, which is able to remove the object smoothly. |
|  015  |  PerSAM  |  Personalize Segment Anything Model with One Shot  |  [Hugging Face](https://huggingface.co/papers/2305.03048)   |  [Code](https://github.com/ZrrSkywalker/Personalize-SAM)    |    -   |  SAM with specific concepts.  |
| 016  |  MedSAM  |  Segment Anything in Medical Images|  -   |  [Code](https://github.com/bowang-lab/MedSAM)    |   -    |    A step-by-step tutorial with a small dataset to help you quickly utilize SAM. |
| 017  |  Segment-Any-Anomaly  |  GroundedSAM Anomaly Detection  |  [Colab](https://colab.research.google.com/drive/1Rwio_KfziuLp79Qh_ugum64Hjnq4ZwsE?usp=sharing)  |  [Code](https://github.com/caoyunkang/Segment-Any-Anomaly)    |   HUST   |    Grounding DINO + SAM to segment any anomaly. |
| 018  |   SSA  |  Semantic Segment Anything  |  -   |  [Code](https://github.com/fudan-zvg/Semantic-Segment-Anything)    |   Fudan University    |    A dense category annotation engine. |
| 019  |  Magic Copy  |  -  |  -   |  [Code](https://github.com/kevmo314/magic-copy)    |   -   |    Magic Copy is a Chrome extension that uses SAM to extract a foreground object from an image and copy it to the clipboard. |
| 020  |  Segment Anything with Clip  |  Segment Anything with Clip  |  [Hugging Face](https://huggingface.co/spaces/curt-park/segment-anything-with-clip)   |  [Code](https://github.com/Curt-Park/segment-anything-with-clip)    |   -    |    SAM combined with CLIP. |
| 021  |  MetaSeg  |  Segment Anything Video|  [Hugging Face](https://huggingface.co/spaces/ArtGAN/Segment-Anything-Video)   |  [Code](https://github.com/kadirnar/segment-anything-video)   |   -    |   Packaged version of the SAM. |
| 022 |  SAM in Napari  |  Segment Anything Model (SAM) in Napari  |  [Project page](https://www.napari-hub.org/plugins/napari-sam)   |  [Code](https://github.com/MIC-DKFZ/napari-sam)    |   Applied Computer Vision Lab and German Cancer Research Center   |    Extended SAM's click-based foreground separation to full click-based semantic segmentation and instance segmentation. |
| 023  |  SAM Medical Imaging  |  SAM Medical Imaging  |  -   | [Code](https://github.com/amine0110/SAM-Medical-Imaging)    |   - | SAM for Medical Imaging.|
|  024  |  3D-Box  |  3D-Box via Segment Anything  |  -   |  [Code](https://github.com/dvlab-research/3D-Box-Segment-Anything)    |   -    |    SAM is extended to 3D perception by combining it with VoxelNeXt. |
| 025  |  Anything-3D  |  -  |  -   |  [Code](https://github.com/Anything-of-anything/Anything-3D)    |   -    |    Anything 3DNovel View, Anything-NeRF, Any 3DFace. |
| 026  |  L2SET  |  Learning to Segment EveryThing  |   -   |  [Code](https://github.com/ronghanghu/seg_every_thing)   |  UC Berkeley, FAIR    |   A new partially supervised training paradigm for instance segmentation. |
| 027  |  Edit Anything  |  Edit Anything by Segment-Anything  |  -   |  [Code](https://github.com/sail-sg/EditAnything)    |   -    |    Edit anything in images powered by SAM, ControlNet, StableDiffusion, \etc. |
| 028  |  Image Edit Anything  |  IEA: Image Editing Anything  |  -   |  [Code](https://github.com/feizc/IEA)    |   -    |  Using stable diffusion and SAM for image editing.   |
| 029  |  SAM for Stable Diffusion Webui  |  Segment Anything for Stable Diffusion WebUI  |  -   |  [Code](https://github.com/continue-revolution/sd-webui-segment-anything)   |   -    |    This extension aim for connecting AUTOMATIC1111 Stable Diffusion WebUI and Mikubill ControlNet Extension with SAM and GroundingDINO to enhance Stable Diffusion/ControlNet inpainting. |
| 030  |  Earth Observation Tools  |  Segment Anything EO tools  |  [Colab](https://colab.research.google.com/drive/1RC1V68tD1O-YissBq9nOvS2PHEjAsFkA?usp=share_link)   |  [Code](https://github.com/aliaksandr960/segment-anything-eo)    |   -    |    An earth observation tools for SAM. |
| 031  |  Moving Object Detection  |  Towards Segmenting Anything That Moves  |  -   |  [Code](https://github.com/achalddave/segment-any-moving) | - | A project about SAM + Moving Object Detection.|
|  032  |  OCR-SAM  |  Optical Character Recognition with Segment Anything  | [Project page](https://www.zhihu.com/question/593914819/answer/2976012032) | [Code](https://github.com/yeungchenwa/OCR-SAM)   |   -    |  Combining MMOCR with SAM and Stable Diffusion. |
| 033  |  SALT  |  Segment Anything Labelling Tool  |  -   |  [Code](https://github.com/anuragxel/salt#segment-anything-labelling-tool-salt)    |   -    |   A project uses the SAM Model and adds a barebones interface to label images and saves the masks in the COCO format. |
|  034  |  Prompt Segment Anything  |  Prompt Segment Anything  |  -   |  [Code](https://github.com/RockeyCoss/Prompt-Segment-Anything)    |   -    |    An implementation of zero-shot instance segmentation using SAM. |
| 035  |  SAM-RBox  |  -  |  -   |  [Code](https://github.com/Li-Qingyun/sam-mmrotate)    |   -    |   A project uses SAM for generating rotated bounding boxes with MMRotate, which is a comparison method of H2RBox-v2. |
| 036  |  VISAM  |  MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors  |-|[Code](https://github.com/BingfengYan/VISAM) |-|   Combining SAM with MOT, it create the era of "MOTS". |
| 037  |  SegEO  |  Segment Anything EO tools  |  -   |  [Code](https://github.com/aliaksandr960/segment-anything-eo)    |   -    |    The tools are developed to ease the processing of spatial data (GeoTIFF and TMS) with SAM using sliding window algorithm for big files. |
| 038  |  Napari Segment Anything |  Napari Segment Anything  |  [Project page](https://app.codecov.io/gh/jookuma/napari-segment-anything)   |  [Code](https://github.com/JoOkuma/napari-segment-anything)    |   -    |    SAM native Qt UI. |
| 039  |  Segment-Anything-U-Specify  |  Segment-Anything-U-Specify  |  -   |  [Code](https://github.com/MaybeShewill-CV/segment-anything-u-specify)    |   -    |  Using CLIP and SAM to segment any instance you specify with text prompt of any instance names. |
|  040  |  SegDrawer  |  Simple static web-based mask drawer  |  [Colab](https://colab.research.google.com/drive/1PdWCpBgYwiQtvkdTBnW-y2T-s_Fc-2iI?usp=sharing)  |  [Code](https://github.com/lujiazho/SegDrawer)    |   -    |    Simple static web-based mask drawer, supporting semantic segmentation with SAM. |
| 041  |  Track Anything  |  Segment Anything Meets Videos  |  [Hugging Face](https://huggingface.co/spaces/VIPLab/Track-Anything)   |  [Code](https://github.com/gaomingqi/Track-Anything)    |   SUSTech    | Track-Anything is a flexible and interactive tool for video object tracking and segmentation. |
| 042  |  Count Anything  |  -  |  -   |  [Code](https://github.com/ylqi/Count-Anything)   |   -    |    A method uses SAM and CLIP to ground and count any object that matches a custom text prompt, without requiring any point or box annotation. |
|  043  |  RAM  |  Relate Anything Model   |  [Hugging Face](https://huggingface.co/spaces/mmlab-ntu/relate-anything-model)   |  [Code](https://github.com/Luodian/RelateAnything)    |   MMLab, NTU and VisCom Lab, KCL/TongJi    |    Relate Anything Model is capable of taking an image as input and utilizing SAM to identify the corresponding mask within the image. |
| 044  |  Segment Any RGBD  |  Segment Any RGBD  |  [Project page](https://github.com/Jun-CEN/SegmentAnyRGBD)   |   [Code](https://huggingface.co/spaces/jcenaa/Segment-Any-RGBD)   |   -    |   Segment AnyRGBD is a toolbox to segment rendered depth images based on SAM. |
|  045  |  Show Anything  |  Show Anything  |  [Hugging Face](https://huggingface.co/spaces/weijiawu/ImageEditAnything)  |  [Code](https://github.com/showlab/ShowAnything)    |   Showlab, NUS    |    Some Applications that are compatible with both SAM and Generation. |
| 046  |  Transfer Any Style|  Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate  |  -   |  [Code](https://github.com/Huage001/Transfer-Any-Style)    |   LV-lab, NUS    |   An interactive demo based on Segment-Anything for style transfer which enables different content regions apply different styles. |
| 047  |  Caption Anything  |  -  |  [Colab](https://colab.research.google.com/github/ttengwang/Caption-Anything/blob/main/notebooks/tutorial.ipynb)  |   [Code](https://github.com/ttengwang/Caption-Anything)   |  VIP lab, SUSTech   |    Caption-Anything is a versatile image processing tool that combines the capabilities of SAM, Visual Captioning, and ChatGPT.  |
|  048  |  Image2Paragraph  |  Transform Image Into Unique Paragraph  |  [Project page](https://zhaohengyuan1.github.io/image2paragraph.github.io/)  |  [Code](https://github.com/showlab/Image2Paragraph)  |  -  |  Transform Image into Unique Paragraph with ChatGPT, BLIP2, OFA, GRIT, Segment Anything, ControlNet. |
| 049    |  LIME SAM  |  Local Interpretable Model-agnostic Explanations Segment Anything  |  [Colab](https://colab.research.google.com/drive/1bj6B-O47NHpqsWovOrVZcpWNhIfO56sj?usp=sharing)  |  [Code](https://github.com/jaydeep-work/LIME-SAM)  |  -  | LIME-SAM aims to create an Explainable Artificial Intelligence (XAI) framework for image classification using LIME (Local Interpretable Model-agnostic Explanations) as the base algorithm, with the super-pixel method replaced by SAM. |
|  050  |  Paint Anything  |  -  |  -  |  [Code](https://github.com/Huage001/Paint-Anything)  |  -  |  An interactive demo based on SAM for stroke-based painting which enables human-like painting. |
| 051  |  SAMed  |  Customized Segment Anything Model for Medical Image Segmentation  |  [Colab](https://colab.research.google.com/drive/1KCS5ulpZasYl9DgJJn59WsGEB8vwSI_m?usp=sharing)  |  [Code](https://github.com/hitachinsk/SAMed)  |  USTC  |  SAMed is built upon the large-scale image segmentation model, SAM, to explore the new research paradigm of customizing large-scale models for medical image segmentation. |
| 052  |  Personalize SAM  |  Personalize Segment Anything with 1 Shot in 10 Seconds  |  [Hugging Face](https://huggingface.co/spaces/justin-zk/Personalize-SAM)  |  [Code](https://github.com/ZrrSkywalker/Personalize-SAM)  |  MMLab, CUHK  |  A training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM can segment specific visual concepts. |
| 053  |  Open-vocabulary-Segment-Anything  | Open-vocabulary-Segment-Anything  |  -  |  [Code](https://github.com/ngthanhtin/owlvit_segment_anything)  |  -  |  Combining OwlViT with Segment Anything - Open-vocabulary Detection and Segmentation (Text-conditioned, and Image-conditioned). |
| 054  |  Labal-Anything-Pipeline  |  Label-Anything-Pipeline |  -  |  [Code](https://github.com/Yuqifan1117/Labal-Anything-Pipeline)  |  ZJU  |  Annotation anything in visual tasks just all in one-pipeline with GPT-4 and SAM. |
| 055  |  Grounded-Segment-Any-Parts  |  Grounded Segment Anything: From Objects to Parts  |  [Project page](https://cheems-seminar.github.io/)  |  [Code](https://github.com/Cheems-Seminar/grounded-segment-any-parts)  |  HKU  |  Expand Segment Anything Model (SAM) to support text prompt input. The text prompt could be object-level(eg, dog) and part-level(eg, dog head).  |
| 056  |  AnyLabeling  |  AnyLabeling  |  [Youtube page](https://www.youtube.com/watch?v=5qVJiYNX5Kk)  |  [Code](https://github.com/vietanhdev/anylabeling)  |  -  |  Effortless AI-assisted data labeling with AI support from Segment Anything and YOLO. |
| 057  |  SSA |  Semantic-Segment-Anything  |  [Project page](https://replicate.com/cjwbw/semantic-segment-anything)  |  [Code](https://github.com/fudan-zvg/Semantic-Segment-Anything)  |  -  |  Automated dense category annotation engine that serves as the initial semantic labeling for the Segment Anything dataset (SA-1B). |
| 058  |  RefSAM |  Label Data with Segment Anything in Roboflow  |  [Project page](https://blog.roboflow.com/label-data-segment-anything-model-sam/)   |  [Code](https://github.com/helblazer811/RefSAM)  |  -  |  Referring Image Segmentation Benchmarking with Segment Anything Model (SAM). |
| 059  |  Roboflow Annotate |  Launch: Label Data with Segment Anything in Roboflow  |  [Project page](https://blog.roboflow.com/label-data-segment-anything-model-sam/)  |  [APP](https://app.roboflow.com/)  |  Roboflow  |  SAM-assisted labeling for training computer vision models. |
| 060  |  ImageBind SAM |  -  |  -  |  [Code](https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/playground/ImageBind_SAM)  |  IDEA-Research  |  This is an experimental demo aims to combine ImageBind and SAM to generate mask with different modalities. |
| 061  | X-AnyLabeling	|   X-AnyLabeling	|  [WeChat](https://mp.weixin.qq.com/s/Fi7i4kw0n_QsA7AgmtP-JQ) | 	[Code](https://github.com/CVHub520/X-AnyLabeling)	| CVHub	| A new interactive automatic labeling tool based on AnyLabeling.|
| 062  |Segment Anything + NNCF |	-	| [WeChat](https://mp.weixin.qq.com/s/LrUx1HEYPCU41k5VZDa7fg) |	[Code](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/237-segment-anything/237-segment-anything.ipynb)	 |  -	 | OpenVINO™ NNCF for segment anything encoder quantization acceleration.|
| 063  |YOLOv8 + SAM	|  - | 	[WeChat](https://mp.weixin.qq.com/s/-rbsvGfc-Q8LZANzUWwn-A) |	-	| - |	Use SAM in YOLOv8. |
| 064  |SearchAnything|  SearchAnything | [Zhihu blog](https://zhuanlan.zhihu.com/p/641128049), [Twitter](https://twitter.com/jd92wang/status/1676114619168067585)	 |[Code](https://github.com/Immortalise/SearchAnything)	| CAS and MSRA |	A semantic local search engine powered by various AI models. |
| 065  |SAM Meets Stable Diffusion 	|  - | 	[WeChat](https://mp.weixin.qq.com/s/HBl-PBdi4Hi6z4Z2FjiYIA) |	[Code](https://aistudio.baidu.com/aistudio/projectdetail/6300584)	| PaddlePaddle |	Segment and generate Anything. |
| 066  |Language Segment-Anything| - | - | [Code]() | - | SAM with text prompts generates masks for specific objects in images. |

## Awesome Repositories for SAM
- [VainF/Awesome-Anything](https://github.com/VainF/Awesome-Anything)
- [Hedlen/Awesome Segment Anything](https://github.com/Hedlen/awesome-segment-anything)
- [Vision-Intelligence-and-Robots-Group/Awesome-Segment-Anything](https://github.com/Vision-Intelligence-and-Robots-Group/Awesome-Segment-Anything)
- [JerryX1110/Awesome-segment-anything-extensions](https://github.com/JerryX1110/awesome-segment-anything-extensions)
- [dk-liang/Awesome-Segment-Anything](https://github.com/dk-liang/Awesome-Segment-Anything)

## Citing SAM Survey

If this survey is useful in your research, please use the following BibTeX entry.

```
@article{chunhui2023samsurvey,
  title={A Comprehensive Survey on Segment Anything Model for Vision and Beyond},
  author={Zhang, Chunhui and Liu, Li and Cui, Yawen and Huang, Guanjie and Lin, Weilin and Yang, Yiqian and Hu, Yuehong},
  journal={arXiv:2304.02643},
  year={2023}
}
```

